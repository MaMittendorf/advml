{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "id": "bTRE9bgcDawa",
    "outputId": "bf049207-2eb2-435c-e44b-c343f6d18e1d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, TimeDistributed, multiply, Lambda, InputLayer\n",
    "from tensorflow.keras.layers import LSTM, GRU, LeakyReLU, BatchNormalization, ReLU, concatenate, Masking, Permute, Reshape\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, RepeatVector,Bidirectional, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "np.random.seed(73) \n",
    "\n",
    "def getmodel(n1, dp1, ac, opt, los, learn):\n",
    "  mcdp = True\n",
    "  \n",
    "  if ac == 1:\n",
    "    ac = 'relu'\n",
    "  elif ac ==2:\n",
    "    ac = 'selu'\n",
    "  else: \n",
    "    ac = 'elu'\n",
    "\n",
    "  if los ==1:\n",
    "    loss = 'mse'\n",
    "  elif los ==2:\n",
    "    loss = 'mae'\n",
    "  else: \n",
    "    loss = 'huber'\n",
    "  \n",
    "  if opt == 1:\n",
    "    opt = Adam(lr=learn)\n",
    "  elif opt == 2:\n",
    "    opt = Nadam(lr=learn)\n",
    "  else: \n",
    "    opt = RMSprop(lr = learn)\n",
    "  input = Input(shape=(14,))\n",
    "  dense = Dense(n1, activation=ac)(input)\n",
    "  dense = Dropout(dp1)(dense, training=mcdp)\n",
    "  dense = Dense(n1,activation=ac)(dense)\n",
    "  dense = Dropout(dp1)(dense, training=mcdp)\n",
    "  dense = Dense(n1,activation=ac)(dense)\n",
    "  dense = Dropout(dp1)(dense, training=mcdp)\n",
    "  output1 = Dense(1, activation='linear')(dense)\n",
    "\n",
    "  model = Model(inputs= input, outputs=output1)\n",
    "  #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "  #model.summary()\n",
    "  print(dp1, ac, loss, opt, learn, n1)\n",
    "  model.compile(optimizer=opt, loss=loss)\n",
    "  return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "GYyfHtWHNB5u",
    "outputId": "b10356ee-29c4-4fb7-920d-67794d93ce02"
   },
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "  q1 = df_in[col_name].quantile(0.15)\n",
    "  q3 = df_in[col_name].quantile(0.85)\n",
    "  iqr = q3-q1 \n",
    "  fence_low  = q1-1.5*iqr\n",
    "  fence_high = q3+1.5*iqr\n",
    "  df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "  return df_out\n",
    "\n",
    "df = pd.read_csv('ropaxdata.csv', sep=',')\n",
    "namarr = ['target','fuelDensity.csv','fuelTemp.csv','inclinometer-raw.csv', 'longitudinalWaterSpeed.csv','level1median.csv','level2median.csv', 'portPitch.csv', 'portRudder.csv','speedKnots.csv','starboardPitch.csv', 'starboardRudder.csv','trueHeading.csv', 'windAngle.csv', 'windSpeed.csv']\n",
    "df.dropna(inplace=True)\n",
    "df = df[namarr]\n",
    "chosen_idx = np.random.choice(610406, replace=False, size=50)\n",
    "df_trimmed = df.iloc[chosen_idx]\n",
    "#print(df.isnull().sum())\n",
    "\n",
    "for nam in namarr: \n",
    "  df = remove_outlier(df,nam)\n",
    "print(df.shape)\n",
    "df = df.sample(frac=0.5, replace=False, random_state=42, axis=0)\n",
    "print(df.shape)\n",
    "#sns.pairplot(data = df, vars=namarr)\n",
    "#plt.show()\n",
    "bounds = [{'name': 'l1',                'type': 'discrete',    'domain': (50,75,100)},\n",
    "          {'name': 'l1_drop',          'type': 'continuous',  'domain': (0.2, 0.5)},\n",
    "          {'name': 'learn',       'type': 'discrete',    'domain': (1e-2,1e-3,1e-4)}]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9L075fm-F3_E",
    "outputId": "75aec432-8428-41f3-c534-3e33c5648645"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import GPy, GPyOpt\n",
    "import tqdm\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "print(df.shape)\n",
    "X = df.drop(['target'], axis=1).to_numpy()\n",
    "Y = df['target'].to_numpy()\n",
    "#X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)\n",
    "#Y = np.divide(np.subtract(Y, Y.mean()), Y.std())\n",
    "Y = np.divide(np.subtract(Y, np.min(Y)), np.max(Y)-np.min(Y))\n",
    "Xtest, Xpool, ytest, ypool = sklearn.model_selection.train_test_split(X,Y,test_size=0.7, shuffle=True)\n",
    "#Xptrain, Xptest, yptrain, yptest = model_selection.train_test_split(X,Y,test_size=0.3, shuffle=True)\n",
    "addn=5000\n",
    "order=np.random.permutation(range(len(Xpool)))\n",
    "poolidx=np.arange(len(Xpool),dtype=np.int)\n",
    "ninit = 100000 #initial samples\n",
    "#initial training set\n",
    "trainset=order[:ninit]\n",
    "Xtrain=np.take(Xpool,trainset,axis=0)\n",
    "ytrain=np.take(ypool,trainset,axis=0)\n",
    "#remove data from pool\n",
    "poolidx=np.setdiff1d(poolidx,trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eZFoYVpFyBrW",
    "outputId": "2d8f5bcb-b2b3-47ad-cbcc-06ba3deda009"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def uncerts(x):\n",
    "  testacc_al=[]\n",
    "  trainset=order[:ninit]\n",
    "  Xtrain=np.take(Xpool,trainset,axis=0)\n",
    "  ytrain=np.take(ypool,trainset,axis=0)\n",
    "  poolidx=np.arange(len(Xpool),dtype=np.int)\n",
    "  poolidx=np.setdiff1d(poolidx,trainset)\n",
    "  model = getmodel(int(x[0]),  float(x[1]),  int(x[2]), int(x[3]),  int(x[4]),  float(x[5]))\n",
    "  for i in range(20):\n",
    "    #fit model\n",
    "    history = model.fit(Xtrain,ytrain,batch_size=int(x[6]), epochs=int(x[7]), verbose=0) \n",
    "    r2_arr = []\n",
    "    pred = []\n",
    "    #predict on test set\n",
    "    for j in tqdm.tqdm(range(50)):\n",
    "      yhat = model.predict(Xtest, batch_size=int(x[6]))\n",
    "      r2_arr.append(sklearn.metrics.r2_score(ytest,yhat))\n",
    "      yhat2 = model.predict(Xpool[poolidx], batch_size=128)\n",
    "      pred.append(yhat2)\n",
    "    eval = np.mean(r2_arr)\n",
    "    print(eval)\n",
    "    print(len(Xtrain))\n",
    "    pred = np.array(np.squeeze(pred)).T\n",
    "    std = []\n",
    "    for rw in pred:\n",
    "      std.append(np.std(rw))\n",
    "    std = np.array(std)\n",
    "    testacc_al.append((len(Xtrain),eval))\n",
    "    #select least confident max likely label - then sort in negative order - note the minus\n",
    "    ypool_p_sort_idx = np.argsort(std)\n",
    "    #add to training set\n",
    "    Xtrain=np.concatenate((Xtrain,Xpool[poolidx[ypool_p_sort_idx[-addn:]]]))\n",
    "    ytrain=np.concatenate((ytrain,ypool[poolidx[ypool_p_sort_idx[-addn:]]]))\n",
    "    #remove from pool\n",
    "    poolidx=np.setdiff1d(poolidx,ypool_p_sort_idx[-addn:])\n",
    "  return model, testacc_al\n",
    "\n",
    "\n",
    "model, test = uncerts([5.00e+01, 1.00e-01, 1.00e+00, 3.00e+00, 1.00e+00, 1.00e-03, 1.28e+02, 3.80e+01])  \n",
    "model.save('uncer.h5')\n",
    "print(test)\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "opt = GPyOpt.methods.BayesianOptimization(f=uncerts, domain=bounds, maximize=True)\n",
    "\n",
    "# optimize  model\n",
    "opt.run_optimization(max_iter=5)\n",
    "print(opt.x_opt, opt.fx_opt)\n",
    "end = time.time()\n",
    "print(end - start)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Bx5AFZ8mQMQZ",
    "outputId": "cbff649f-d80d-4d06-98d2-4f16f5ca50af"
   },
   "outputs": [],
   "source": [
    "print(test)\n",
    "model.save('uncertain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ds5KAVN5dQp4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "uncertainty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
